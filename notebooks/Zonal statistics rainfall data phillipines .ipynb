{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import geopandas as gpd\n",
    "import rasterio \n",
    "import rasterio.mask\n",
    "import os \n",
    "import sys \n",
    "from tqdm.notebook import tqdm \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth \n",
    "import zipfile \n",
    "\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime \n",
    "from datetime import date \n",
    "\n",
    "sys.path.append(\"/Users/mishaklein/Documents/redcross_510\")\n",
    "from redcross510_utils import * \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enter the stuff that changes over here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = str(date.today())\n",
    "download_dir = './data_download'\n",
    "zonal_stats_file_name = f\"PHL_admin3_zonal_statistics_{today.replace('-','_')}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_typhoons={'bopha':'2012-12-04 04:45:00',\n",
    "                'durian':'2006-11-30 00:00:00',\n",
    "                'fengshen':'2008-06-21 12:00:00',\n",
    "                'ketsana':'2009-09-26 00:00:00',\n",
    "                'washi':'2011-12-16 00:00:00',\n",
    "                'haiyan':'2013-11-08 00:00:00',\n",
    "                'hagupit':'2014-12-06 23:00:00',\n",
    "                'haima':'2016-10-19 23:00:00',\n",
    "                'nock-ten':'2016-12-25 18:00:00',\n",
    "                'mangkhut':'2018-09-15 01:40:00',\n",
    "                'kammuri':'2019-12-02 20:00:00',\n",
    "                'phanfone':'2019-12-24 00:00:00',\n",
    "                'vongfong':'2020-05-14 00:00:00',\n",
    "                'molave':'2020-10-25 18:10:00',\n",
    "                'goni':'2020-11-01 05:00:00'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading data from the web "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NASA_EOSDIS_file(year,month,day, username, password):\n",
    "    '''\n",
    "    \n",
    "    Access the download page of the Earth Observing System Data and Information System (EOSDIS)\n",
    "    and get return the download link for the daily data (.zip)\n",
    "    '''\n",
    "    # Go to page \n",
    "    baseURL = \"https://arthurhouhttps.pps.eosdis.nasa.gov/gpmallversions/V06/\"\n",
    "    page = requests.get(url = os.path.join(baseURL, year, month, day, 'gis'),\n",
    "                       auth=HTTPBasicAuth(username, password))\n",
    "    \n",
    "    \n",
    "    # BeautifulSoup package has very convinient ways to search through such content \n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    for a in soup.find_all('a'):\n",
    "        link = a.get('href')\n",
    "        # We know the prefix of the file name and it's extension\n",
    "        if link.startswith(\"3B-DAY-GIS\") and link.endswith(\".zip\"):\n",
    "            download_file = link \n",
    "    return os.path.join(baseURL, year, month, day, 'gis', download_file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def download_rainfall_typhoons_phillipines(typhoons_of_interest = {}, \n",
    "                                           download_dir = './data_download',\n",
    "                                           username = \"akliludin@gmail.com\",\n",
    "                                           password = \"akliludin@gmail.com\"):\n",
    "    '''\n",
    "    input a dictionary with keys of typhoon names and values the dates in \"year-month-day hour:minutes:seconds\" format\n",
    "    this will download the daily rainfall data, and unzip the obtained zip-archives \n",
    "    '''\n",
    "    \n",
    "    baseURL = \"https://arthurhouhttps.pps.eosdis.nasa.gov/gpmallversions/V06/\"\n",
    "    \n",
    "    download_links = []\n",
    "    for typhoon in tqdm(typhoons_of_interest): \n",
    "        \n",
    "        # use datetime object to handle the date, month, year, wich we'll need to get the final download link\n",
    "        date_of_typhoon = datetime.strptime(typhoons_of_interest[typhoon], '%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        year = str(date_of_typhoon.year) \n",
    "        if date_of_typhoon.month < 10:\n",
    "            month = \"0\" + str(date_of_typhoon.month)\n",
    "        else: \n",
    "            month = str(date_of_typhoon.month)\n",
    "\n",
    "        if date_of_typhoon.day < 10:\n",
    "            day = \"0\" + str(date_of_typhoon.day)\n",
    "        else: \n",
    "            day = str(date_of_typhoon.day)\n",
    "        \n",
    "        \n",
    "        # get the complete link to the zipfiles to be downloaded\n",
    "        fullURL = get_NASA_EOSDIS_file(year,month,day, username, password)\n",
    "\n",
    "        # download files \n",
    "        download_file = os.path.join(download_dir, fullURL.split('/')[-1])\n",
    "        \n",
    "        # if not already donwloaded: \n",
    "        if not os.path.join(download_dir, typhoon) + \".zip\":\n",
    "            download_files_url(url = fullURL, username = username, password = password, \n",
    "                               path_download_file = download_file)\n",
    "\n",
    "            # unzip into new folder \n",
    "            output_folder = os.path.join(download_dir, typhoon)\n",
    "            if not os.path.exists( output_folder):\n",
    "                os.makedirs( output_folder )\n",
    "\n",
    "\n",
    "            new_name = os.path.join(download_dir, typhoon) + \".zip\"\n",
    "            os.rename(download_file, new_name)\n",
    "            extract_zip_archive(zip_archive = new_name, destination_dir = output_folder)\n",
    "            \n",
    "        \n",
    "        # create log file (to know what happened)\n",
    "        download_links.append(fullURL)\n",
    "    \n",
    "    # create log file (to know what happened)\n",
    "    today = str(date.today())\n",
    "    log_file_name = os.path.join(download_dir, '_'.join(['info_downloads', today.replace('-','_')]) + '.csv')\n",
    "    logFile = pd.DataFrame(typhoons_of_interest, columns = [\"typhoon_name\", \"date\"])\n",
    "    logFile['downloaded_from'] = download_links \n",
    "    logFile['downloaded_to'] = download_dir\n",
    "    logFile.to_csv(log_file_name, index = False)\n",
    "    print(\"downloads completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee32a3864a13471dac24febb839c332e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloads completed\n"
     ]
    }
   ],
   "source": [
    "download_rainfall_typhoons_phillipines(typhoons_of_interest = dict_typhoons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zonal Statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gridid</th>\n",
       "      <th>glat</th>\n",
       "      <th>glon</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PH01280100</td>\n",
       "      <td>18.453</td>\n",
       "      <td>120.919</td>\n",
       "      <td>POINT (120.91941 18.45339)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PH01280200</td>\n",
       "      <td>18.267</td>\n",
       "      <td>120.613</td>\n",
       "      <td>POINT (120.61290 18.26725)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PH01280300</td>\n",
       "      <td>17.906</td>\n",
       "      <td>120.509</td>\n",
       "      <td>POINT (120.50897 17.90596)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PH01280400</td>\n",
       "      <td>18.490</td>\n",
       "      <td>120.750</td>\n",
       "      <td>POINT (120.74967 18.48975)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PH01280500</td>\n",
       "      <td>18.038</td>\n",
       "      <td>120.584</td>\n",
       "      <td>POINT (120.58423 18.03775)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5672</th>\n",
       "      <td>PH18462200</td>\n",
       "      <td>9.944</td>\n",
       "      <td>123.075</td>\n",
       "      <td>POINT (123.07455 9.94447)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5673</th>\n",
       "      <td>PH18462300</td>\n",
       "      <td>9.291</td>\n",
       "      <td>123.161</td>\n",
       "      <td>POINT (123.16082 9.29122)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5674</th>\n",
       "      <td>PH18462400</td>\n",
       "      <td>10.279</td>\n",
       "      <td>123.331</td>\n",
       "      <td>POINT (123.33148 10.27914)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5675</th>\n",
       "      <td>PH18462400</td>\n",
       "      <td>10.343</td>\n",
       "      <td>123.293</td>\n",
       "      <td>POINT (123.29275 10.34337)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5676</th>\n",
       "      <td>PH18462500</td>\n",
       "      <td>9.162</td>\n",
       "      <td>123.162</td>\n",
       "      <td>POINT (123.16247 9.16242)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5677 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          gridid    glat     glon                    geometry\n",
       "0     PH01280100  18.453  120.919  POINT (120.91941 18.45339)\n",
       "1     PH01280200  18.267  120.613  POINT (120.61290 18.26725)\n",
       "2     PH01280300  17.906  120.509  POINT (120.50897 17.90596)\n",
       "3     PH01280400  18.490  120.750  POINT (120.74967 18.48975)\n",
       "4     PH01280500  18.038  120.584  POINT (120.58423 18.03775)\n",
       "...          ...     ...      ...                         ...\n",
       "5672  PH18462200   9.944  123.075   POINT (123.07455 9.94447)\n",
       "5673  PH18462300   9.291  123.161   POINT (123.16082 9.29122)\n",
       "5674  PH18462400  10.279  123.331  POINT (123.33148 10.27914)\n",
       "5675  PH18462400  10.343  123.293  POINT (123.29275 10.34337)\n",
       "5676  PH18462500   9.162  123.162   POINT (123.16247 9.16242)\n",
       "\n",
       "[5677 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_shapes = \"/Users/mishaklein/Documents/redcross_510/shapefiles/phl_admin3_version2/phl_admin3_version2.shp\"\n",
    "\n",
    "shapeData = gpd.read_file(file_shapes)\n",
    "shapeData "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I do this to check the column name that has either the region's name or the p-code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### perform zonal statistics for every typhoon (one TIFF file) and merge together into one final output table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "944d6cd7d08246348b636ed9a6a8cc28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote file: PHL_admin3_zonal_statistics_2021_05_23.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>max</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pcode</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PH01280100</th>\n",
       "      <td>221.666667</td>\n",
       "      <td>1735</td>\n",
       "      <td>3325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PH01280200</th>\n",
       "      <td>193.066667</td>\n",
       "      <td>910</td>\n",
       "      <td>2896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PH01280300</th>\n",
       "      <td>214.933333</td>\n",
       "      <td>1275</td>\n",
       "      <td>3224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PH01280400</th>\n",
       "      <td>304.466667</td>\n",
       "      <td>1901</td>\n",
       "      <td>4567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PH01280500</th>\n",
       "      <td>210.400000</td>\n",
       "      <td>1103</td>\n",
       "      <td>3156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PH18462100</th>\n",
       "      <td>183.533333</td>\n",
       "      <td>547</td>\n",
       "      <td>5506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PH18462200</th>\n",
       "      <td>228.800000</td>\n",
       "      <td>580</td>\n",
       "      <td>3432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PH18462300</th>\n",
       "      <td>185.933333</td>\n",
       "      <td>699</td>\n",
       "      <td>2789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PH18462400</th>\n",
       "      <td>278.133333</td>\n",
       "      <td>855</td>\n",
       "      <td>8344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PH18462500</th>\n",
       "      <td>212.000000</td>\n",
       "      <td>805</td>\n",
       "      <td>3180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1647 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  mean   max   sum\n",
       "pcode                             \n",
       "PH01280100  221.666667  1735  3325\n",
       "PH01280200  193.066667   910  2896\n",
       "PH01280300  214.933333  1275  3224\n",
       "PH01280400  304.466667  1901  4567\n",
       "PH01280500  210.400000  1103  3156\n",
       "...                ...   ...   ...\n",
       "PH18462100  183.533333   547  5506\n",
       "PH18462200  228.800000   580  3432\n",
       "PH18462300  185.933333   699  2789\n",
       "PH18462400  278.133333   855  8344\n",
       "PH18462500  212.000000   805  3180\n",
       "\n",
       "[1647 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapefile = \"/Users/mishaklein/Documents/redcross_510/shapefiles/phl_admin3_version2/phl_admin3_version2.shp\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "zonalStats = pd.DataFrame()\n",
    "for typhoon in tqdm(dict_typhoons): \n",
    "    data_dir = os.path.join(download_dir, typhoon)\n",
    "    filenames = [f for f in os.listdir(data_dir) if f.endswith('total.accum.tif')]\n",
    "    assert len(filenames) == 1\n",
    "    rasterfile =  os.path.join(download_dir, typhoon, filenames[0])\n",
    "    \n",
    "    oneTyphoon = zonal_statistics(rasterfile,shapefile, pcodeKey = \"gridid\",\n",
    "                                 aggregate_by=[np.mean, np.max, np.sum])\n",
    "    oneTyphoon['typhoon_name'] = typhoon \n",
    "    \n",
    "    zonalStats = pd.concat([zonalStats, oneTyphoon])\n",
    "    \n",
    "\n",
    "# THIS PART IS SPECIFIC TO THIS DATA SET: \n",
    "# pcodes did not correspond to unique admin boundaries. So we here should aggregate by pcode\n",
    "rename_dict = {\n",
    "    'value_1':'mean',\n",
    "    'value_2':'max',\n",
    "    'value_3':'sum'\n",
    "}\n",
    "\n",
    "zonalStats.rename(mapper = rename_dict, axis = 'columns', inplace = True)\n",
    "\n",
    "typhoonData = pd.DataFrame()\n",
    "grouped =   zonalStats.groupby(by = ['pcode'])\n",
    "typhoonData['mean'] = grouped['mean'].agg(np.mean)\n",
    "typhoonData['max'] = grouped['max'].agg(np.max)\n",
    "typhoonData['sum'] = grouped['sum'].agg(np.sum)\n",
    "\n",
    "    \n",
    "typhoonData.to_csv(zonal_stats_file_name, index = False)\n",
    "print(f\"wrote file: {zonal_stats_file_name}\")\n",
    "typhoonData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GeoRedCross",
   "language": "python",
   "name": "georedcross"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
